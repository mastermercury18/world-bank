{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ag2TPcA2kzbI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "rainfall = np.random.normal(loc=50, scale=15, size=1000)  #mm\n",
        "ndvi = np.random.normal(loc=0.5, scale=0.1, size=1000)     #veg health\n",
        "temperature = np.random.normal(loc=30, scale=5, size=1000) #deg C\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qiskit.algorithms import EstimationProblem\n",
        "from qiskit.algorithms.amplitude_estimators import IterativeAmplitudeEstimation\n",
        "from qiskit.circuit.library import GroverOperator\n",
        "from qiskit.primitives import Sampler\n",
        "from qiskit import QuantumCircuit\n",
        "import numpy as np\n",
        "\n",
        "def construct_amplitude_circuit(threshold=40):\n",
        "    qc = QuantumCircuit(1)\n",
        "    qc.ry(np.pi/4, 0)  #encoding rainfall as rotation\n",
        "    return qc\n",
        "\n",
        "qc = construct_amplitude_circuit()\n",
        "problem = EstimationProblem(state_preparation=qc)\n",
        "\n",
        "sampler = Sampler()\n",
        "iae = IterativeAmplitudeEstimation(epsilon_target=0.01, alpha=0.05, sampler=sampler)\n",
        "\n",
        "result = iae.estimate(problem)\n",
        "print(\"Estimated expected payout:\", result.estimation)"
      ],
      "metadata": {
        "id": "7HF52sSdk851"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pymc as pm\n",
        "\n",
        "with pm.Model() as model:\n",
        "    theta = pm.Normal(\"theta\", mu=0.4, sigma=0.1)\n",
        "    x_obs = pm.Normal(\"x_obs\", mu=theta, sigma=0.05, observed=rainfall.mean()/100)\n",
        "    trace = pm.sample(1000, tune=500)\n",
        "\n",
        "posterior_mean = trace.posterior[\"theta\"].mean().values\n",
        "print(\"Posterior mean of theta:\", posterior_mean)"
      ],
      "metadata": {
        "id": "KD9ntldZlE4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dwave.system import LeapHybridSampler\n",
        "import dimod\n",
        "\n",
        "Q = {\n",
        "    ('r1', 'r1'): 2.1 - posterior_mean,  #penalize overpricing\n",
        "    ('r2', 'r2'): 1.8,\n",
        "    ('r1', 'r2'): -0.3,\n",
        "    ('l1', 'l1'): 2.0,  #loading fees\n",
        "    ('b1', 'b1'): 1.6,  #buffer\n",
        "}\n",
        "\n",
        "bqm = dimod.BinaryQuadraticModel.from_qubo(Q)\n",
        "sampler = LeapHybridSampler()\n",
        "results = sampler.sample(bqm)\n",
        "\n",
        "print(\"Best premium config:\", results.first.sample)"
      ],
      "metadata": {
        "id": "j9Sq2fvdlHLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quantum-Enhanced Dynamic Pricing for Climate Risk Insurance\n",
        "# ------------------------------------------------------------\n",
        "# Developed by Team Quantum HQ for the World Bank Quantum Industry Challenge\n",
        "\n",
        "# 1. Imports\n",
        "import numpy as np\n",
        "import pymc as pm\n",
        "from matplotlib import pyplot as plt\n",
        "!pip install qiskit\n",
        "!pip install dwave\n",
        "!pip install dimod\n",
        "!pip install dwave-ocean-sdk\n",
        "!pip install qiskit-aer\n",
        "\n",
        "from qiskit import QuantumCircuit, Aer, execute\n",
        "from dwave.system import LeapHybridSampler\n",
        "import dimod\n",
        "\n",
        "# 2. Simulate environmental data (e.g., rainfall in mm)\n",
        "np.random.seed(42)\n",
        "rainfall_data = np.random.normal(loc=50, scale=15, size=1000)\n",
        "\n",
        "# 3. Bayesian posterior estimation using PyMC\n",
        "with pm.Model() as bayesian_model:\n",
        "    theta = pm.Normal(\"theta\", mu=0.4, sigma=0.1)\n",
        "    x_obs = pm.Normal(\"x_obs\", mu=theta, sigma=0.05, observed=rainfall_data.mean() / 100)\n",
        "    trace = pm.sample(1000, tune=500, progressbar=False)\n",
        "\n",
        "posterior_mean = trace.posterior[\"theta\"].mean().values\n",
        "posterior_mean_scalar = float(posterior_mean)\n",
        "\n",
        "# 4. Quantum Amplitude Estimation Simulation (Qiskit)\n",
        "def construct_amplitude_circuit(threshold=40):\n",
        "    qc = QuantumCircuit(1, 1)\n",
        "    qc.ry(np.pi/4, 0)  # encoding environmental data into amplitude\n",
        "    qc.measure(0, 0)\n",
        "    return qc\n",
        "\n",
        "qc = construct_amplitude_circuit()\n",
        "backend = Aer.get_backend(\"qasm_simulator\")\n",
        "job = execute(qc, backend=backend, shots=1024)\n",
        "result = job.result()\n",
        "counts = result.get_counts()\n",
        "zero_prob = counts.get(\"0\", 0) / 1024\n",
        "qae_estimate = zero_prob  # interpreted as expected payout\n",
        "\n",
        "# 5. QUBO Construction for Premium Pricing\n",
        "Q = {\n",
        "    ('r1', 'r1'): 2.1 - qae_estimate,\n",
        "    ('r2', 'r2'): 1.8 + posterior_mean_scalar,\n",
        "    ('r1', 'r2'): -0.3,\n",
        "    ('l1', 'l1'): 2.0,\n",
        "    ('b1', 'b1'): 1.6\n",
        "}\n",
        "bqm = dimod.BinaryQuadraticModel.from_qubo(Q)\n",
        "\n",
        "# Using classical solver for demo (LeapHybridSampler can be used with token)\n",
        "sampler = dimod.ExactSolver()\n",
        "qubo_results = sampler.sample(bqm)\n",
        "best_sample = qubo_results.first.sample\n",
        "\n",
        "# 6. Visualization\n",
        "plt.hist(rainfall_data, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.axvline(x=40, color='red', linestyle='--', label='Payout Threshold')\n",
        "plt.title(\"Simulated Rainfall Distribution\")\n",
        "plt.xlabel(\"Rainfall (mm)\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 7. Output Summary\n",
        "print(\"Posterior Mean of θ:\", posterior_mean_scalar)\n",
        "print(\"Estimated Payout Probability (QAE):\", qae_estimate)\n",
        "print(\"Optimal Premium Configuration (QUBO):\", best_sample)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uPOXLnrvHdLn",
        "outputId": "cbd8712c-b421-402b-8e01-fe7469269399"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: qiskit in /usr/local/lib/python3.11/dist-packages (2.1.1)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from qiskit) (0.16.0)\n",
            "Requirement already satisfied: numpy<3,>=1.17 in /usr/local/lib/python3.11/dist-packages (from qiskit) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.11/dist-packages (from qiskit) (1.15.3)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.11/dist-packages (from qiskit) (0.3.7)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from qiskit) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qiskit) (4.14.1)\n",
            "Requirement already satisfied: pbr>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from stevedore>=3.0.0->qiskit) (6.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pbr>=2.0.0->stevedore>=3.0.0->qiskit) (75.2.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement dwave (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for dwave\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: dimod in /usr/local/lib/python3.11/dist-packages (0.12.20)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from dimod) (2.0.2)\n",
            "Collecting dwave-ocean-sdk\n",
            "  Downloading dwave_ocean_sdk-8.4.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: dimod==0.12.20 in /usr/local/lib/python3.11/dist-packages (from dwave-ocean-sdk) (0.12.20)\n",
            "Collecting dwave-cloud-client==0.13.6 (from dwave-ocean-sdk)\n",
            "  Downloading dwave_cloud_client-0.13.6-py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting dwave-gate==0.3.4 (from dwave-ocean-sdk)\n",
            "  Downloading dwave_gate-0.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting dwave-hybrid==0.6.14 (from dwave-ocean-sdk)\n",
            "  Downloading dwave_hybrid-0.6.14-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting dwave-inspector==0.5.4 (from dwave-ocean-sdk)\n",
            "  Downloading dwave_inspector-0.5.4-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting dwave-networkx==0.8.18 (from dwave-ocean-sdk)\n",
            "  Downloading dwave_networkx-0.8.18-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting dwave-optimization==0.6.2 (from dwave-ocean-sdk)\n",
            "  Downloading dwave_optimization-0.6.2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (18 kB)\n",
            "Collecting dwave-preprocessing==0.6.9 (from dwave-ocean-sdk)\n",
            "  Downloading dwave_preprocessing-0.6.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
            "Collecting dwave-samplers==1.6.0 (from dwave-ocean-sdk)\n",
            "  Downloading dwave_samplers-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting dwave-system==1.32.0 (from dwave-ocean-sdk)\n",
            "  Downloading dwave_system-1.32.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting dwavebinarycsp==0.3.1 (from dwave-ocean-sdk)\n",
            "  Downloading dwavebinarycsp-0.3.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting minorminer==0.2.19 (from dwave-ocean-sdk)\n",
            "  Downloading minorminer-0.2.19-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (19 kB)\n",
            "Collecting penaltymodel==1.2.0 (from dwave-ocean-sdk)\n",
            "  Downloading penaltymodel-1.2.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from dimod==0.12.20->dwave-ocean-sdk) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.25 in /usr/local/lib/python3.11/dist-packages (from requests[socks]<3,>=2.25->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (2.32.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.11/dist-packages (from dwave-cloud-client==0.13.6->dwave-ocean-sdk) (2.4.0)\n",
            "Requirement already satisfied: pydantic<3,>=2 in /usr/local/lib/python3.11/dist-packages (from dwave-cloud-client==0.13.6->dwave-ocean-sdk) (2.11.7)\n",
            "Collecting homebase<2,>=1.0 (from dwave-cloud-client==0.13.6->dwave-ocean-sdk)\n",
            "  Downloading homebase-1.0.1-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from dwave-cloud-client==0.13.6->dwave-ocean-sdk) (8.2.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7 in /usr/local/lib/python3.11/dist-packages (from dwave-cloud-client==0.13.6->dwave-ocean-sdk) (2.9.0.post0)\n",
            "Collecting plucky<0.5,>=0.4.3 (from dwave-cloud-client==0.13.6->dwave-ocean-sdk)\n",
            "  Downloading plucky-0.4.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting diskcache<6,>=5.2.1 (from dwave-cloud-client==0.13.6->dwave-ocean-sdk)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: packaging>=19 in /usr/local/lib/python3.11/dist-packages (from dwave-cloud-client==0.13.6->dwave-ocean-sdk) (24.2)\n",
            "Requirement already satisfied: werkzeug<4,>=2.2 in /usr/local/lib/python3.11/dist-packages (from dwave-cloud-client==0.13.6->dwave-ocean-sdk) (3.1.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from dwave-cloud-client==0.13.6->dwave-ocean-sdk) (4.14.1)\n",
            "Collecting authlib<2,>=1.2 (from dwave-cloud-client==0.13.6->dwave-ocean-sdk)\n",
            "  Downloading authlib-1.6.0-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: importlib_metadata>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dwave-cloud-client==0.13.6->dwave-ocean-sdk) (8.7.0)\n",
            "Requirement already satisfied: orjson>=3.10 in /usr/local/lib/python3.11/dist-packages (from dwave-cloud-client==0.13.6->dwave-ocean-sdk) (3.10.18)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from dwave-hybrid==0.6.14->dwave-ocean-sdk) (3.5)\n",
            "Requirement already satisfied: Flask<4,>=2.2 in /usr/local/lib/python3.11/dist-packages (from dwave-inspector==0.5.4->dwave-ocean-sdk) (3.1.1)\n",
            "Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.11/dist-packages (from dwave-system==1.32.0->dwave-ocean-sdk) (1.15.3)\n",
            "Collecting fasteners>=0.15 (from minorminer==0.2.19->dwave-ocean-sdk)\n",
            "  Downloading fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.11/dist-packages (from authlib<2,>=1.2->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (43.0.3)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4,>=2.2->dwave-inspector==0.5.4->dwave-ocean-sdk) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from Flask<4,>=2.2->dwave-inspector==0.5.4->dwave-ocean-sdk) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4,>=2.2->dwave-inspector==0.5.4->dwave-ocean-sdk) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4,>=2.2->dwave-inspector==0.5.4->dwave-ocean-sdk) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata>=5.0.0->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (3.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (0.4.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil<3,>=2.7->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.25->requests[socks]<3,>=2.25->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.25->requests[socks]<3,>=2.25->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.25->requests[socks]<3,>=2.25->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (2025.6.15)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]<3,>=2.25->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (1.7.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography->authlib<2,>=1.2->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography->authlib<2,>=1.2->dwave-cloud-client==0.13.6->dwave-ocean-sdk) (2.22)\n",
            "Downloading dwave_ocean_sdk-8.4.0-py3-none-any.whl (8.4 kB)\n",
            "Downloading dwave_cloud_client-0.13.6-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.2/161.2 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dwave_gate-0.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dwave_hybrid-0.6.14-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dwave_inspector-0.5.4-py3-none-any.whl (30 kB)\n",
            "Downloading dwave_networkx-0.8.18-py3-none-any.whl (106 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dwave_optimization-0.6.2-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.7/3.7 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dwave_preprocessing-0.6.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dwave_samplers-1.6.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dwave_system-1.32.0-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.9/116.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dwavebinarycsp-0.3.1-py3-none-any.whl (35 kB)\n",
            "Downloading minorminer-0.2.19-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading penaltymodel-1.2.0-py3-none-any.whl (36 kB)\n",
            "Downloading authlib-1.6.0-py2.py3-none-any.whl (239 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.19-py3-none-any.whl (18 kB)\n",
            "Downloading homebase-1.0.1-py2.py3-none-any.whl (11 kB)\n",
            "Downloading plucky-0.4.3-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: plucky, homebase, fasteners, dwave-optimization, dwave-gate, diskcache, penaltymodel, dwave-samplers, dwave-preprocessing, dwave-networkx, minorminer, dwavebinarycsp, authlib, dwave-cloud-client, dwave-system, dwave-inspector, dwave-hybrid, dwave-ocean-sdk\n",
            "Successfully installed authlib-1.6.0 diskcache-5.6.3 dwave-cloud-client-0.13.6 dwave-gate-0.3.4 dwave-hybrid-0.6.14 dwave-inspector-0.5.4 dwave-networkx-0.8.18 dwave-ocean-sdk-8.4.0 dwave-optimization-0.6.2 dwave-preprocessing-0.6.9 dwave-samplers-1.6.0 dwave-system-1.32.0 dwavebinarycsp-0.3.1 fasteners-0.19 homebase-1.0.1 minorminer-0.2.19 penaltymodel-1.2.0 plucky-0.4.3\n",
            "Collecting qiskit-aer\n",
            "  Downloading qiskit_aer-0.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: qiskit>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from qiskit-aer) (2.1.1)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.11/dist-packages (from qiskit-aer) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.11/dist-packages (from qiskit-aer) (1.15.3)\n",
            "Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.11/dist-packages (from qiskit-aer) (5.9.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from qiskit-aer) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.0->qiskit-aer) (1.17.0)\n",
            "Requirement already satisfied: rustworkx>=0.15.0 in /usr/local/lib/python3.11/dist-packages (from qiskit>=1.1.0->qiskit-aer) (0.16.0)\n",
            "Requirement already satisfied: dill>=0.3 in /usr/local/lib/python3.11/dist-packages (from qiskit>=1.1.0->qiskit-aer) (0.3.7)\n",
            "Requirement already satisfied: stevedore>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from qiskit>=1.1.0->qiskit-aer) (5.4.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from qiskit>=1.1.0->qiskit-aer) (4.14.1)\n",
            "Requirement already satisfied: pbr>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from stevedore>=3.0.0->qiskit>=1.1.0->qiskit-aer) (6.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pbr>=2.0.0->stevedore>=3.0.0->qiskit>=1.1.0->qiskit-aer) (75.2.0)\n",
            "Downloading qiskit_aer-0.17.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: qiskit-aer\n",
            "Successfully installed qiskit-aer-0.17.1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'Aer' from 'qiskit' (/usr/local/lib/python3.11/dist-packages/qiskit/__init__.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-968259130.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install qiskit-aer'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mqiskit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mQuantumCircuit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdwave\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLeapHybridSampler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdimod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'Aer' from 'qiskit' (/usr/local/lib/python3.11/dist-packages/qiskit/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Quantum-Enhanced Dynamic Pricing for Climate Risk Insurance\n",
        "Team: Quantum HQ\n",
        "Phase 3 Implementation - End-to-End Solution\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import stats\n",
        "from datetime import datetime, timedelta\n",
        "import json\n",
        "import sqlite3\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Quantum Computing Imports\n",
        "from qiskit import QuantumCircuit, transpile\n",
        "from qiskit.circuit.library import RealAmplitudes\n",
        "from qiskit_aer import AerSimulator, noise\n",
        "from qiskit.quantum_info import SparsePauliOp\n",
        "from qiskit.primitives import Estimator\n",
        "from qiskit.algorithms import AmplitudeEstimation, EstimationProblem\n",
        "from qiskit.circuit.library import LinearAmplitudeFunction\n",
        "\n",
        "# Classical ML and Optimization\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import dimod\n",
        "from dwave.system import LeapHybridSampler\n",
        "from dwave.samplers import SimulatedAnnealingSampler\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class QuantumClimateInsurance:\n",
        "    \"\"\"\n",
        "    Main class implementing quantum-enhanced dynamic pricing for climate risk insurance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config_path=None):\n",
        "        \"\"\"Initialize the quantum climate insurance system\"\"\"\n",
        "        self.config = self._load_config(config_path)\n",
        "        self.db_path = \"climate_insurance.db\"\n",
        "        self.simulator = AerSimulator()\n",
        "        self.noise_model = None\n",
        "        self.scaler = StandardScaler()\n",
        "        self.pricing_history = []\n",
        "        self.performance_metrics = {}\n",
        "\n",
        "        # Initialize database\n",
        "        self._initialize_database()\n",
        "\n",
        "        # Generate synthetic data for demonstration\n",
        "        self._generate_synthetic_data()\n",
        "\n",
        "    def _load_config(self, config_path):\n",
        "        \"\"\"Load configuration parameters\"\"\"\n",
        "        default_config = {\n",
        "            \"regions\": [\"Region_A\", \"Region_B\", \"Region_C\"],\n",
        "            \"historical_years\": 30,\n",
        "            \"drought_threshold\": 0.25,  # 25th percentile\n",
        "            \"max_payout\": 1000,\n",
        "            \"base_premium\": 100,\n",
        "            \"qae_shots\": 1000,\n",
        "            \"qubo_iterations\": 100,\n",
        "            \"noise_enabled\": True\n",
        "        }\n",
        "\n",
        "        if config_path:\n",
        "            with open(config_path, 'r') as f:\n",
        "                user_config = json.load(f)\n",
        "            default_config.update(user_config)\n",
        "\n",
        "        return default_config\n",
        "\n",
        "    def _initialize_database(self):\n",
        "        \"\"\"Initialize SQLite database for storing data\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        cursor = conn.cursor()\n",
        "\n",
        "        # Create tables\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS climate_data (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                region TEXT,\n",
        "                date DATE,\n",
        "                rainfall REAL,\n",
        "                temperature REAL,\n",
        "                ndvi REAL,\n",
        "                drought_index REAL\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        cursor.execute('''\n",
        "            CREATE TABLE IF NOT EXISTS pricing_results (\n",
        "                id INTEGER PRIMARY KEY,\n",
        "                region TEXT,\n",
        "                date DATE,\n",
        "                expected_loss REAL,\n",
        "                premium REAL,\n",
        "                payout_probability REAL,\n",
        "                quantum_advantage REAL\n",
        "            )\n",
        "        ''')\n",
        "\n",
        "        conn.commit()\n",
        "        conn.close()\n",
        "\n",
        "    def _generate_synthetic_data(self):\n",
        "        \"\"\"Generate synthetic climate data for demonstration\"\"\"\n",
        "        np.random.seed(42)\n",
        "\n",
        "        # Generate 30 years of historical data + 1 year of current data\n",
        "        start_date = datetime(1994, 1, 1)\n",
        "        end_date = datetime(2024, 12, 31)\n",
        "\n",
        "        dates = pd.date_range(start_date, end_date, freq='D')\n",
        "\n",
        "        climate_data = []\n",
        "\n",
        "        for region in self.config[\"regions\"]:\n",
        "            for date in dates:\n",
        "                # Simulate seasonal patterns and climate change trends\n",
        "                day_of_year = date.timetuple().tm_yday\n",
        "                year = date.year\n",
        "\n",
        "                # Rainfall (log-normal distribution with seasonal variation)\n",
        "                seasonal_factor = 1 + 0.5 * np.sin(2 * np.pi * day_of_year / 365)\n",
        "                climate_trend = 1 - 0.001 * (year - 1994)  # Slight decreasing trend\n",
        "\n",
        "                rainfall = np.random.lognormal(\n",
        "                    mean=np.log(50 * seasonal_factor * climate_trend),\n",
        "                    sigma=0.5\n",
        "                )\n",
        "\n",
        "                # Temperature (normal distribution with seasonal variation)\n",
        "                temp_base = 20 + 10 * np.sin(2 * np.pi * day_of_year / 365)\n",
        "                temperature = np.random.normal(temp_base, 5)\n",
        "\n",
        "                # NDVI (correlated with rainfall)\n",
        "                ndvi = np.clip(0.3 + 0.4 * (rainfall / 100) + np.random.normal(0, 0.1), 0, 1)\n",
        "\n",
        "                # Drought index (inverse of rainfall percentile)\n",
        "                drought_index = max(0, 1 - rainfall / 100)\n",
        "\n",
        "                climate_data.append({\n",
        "                    'region': region,\n",
        "                    'date': date.strftime('%Y-%m-%d'),\n",
        "                    'rainfall': rainfall,\n",
        "                    'temperature': temperature,\n",
        "                    'ndvi': ndvi,\n",
        "                    'drought_index': drought_index\n",
        "                })\n",
        "\n",
        "        # Store in database\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        df = pd.DataFrame(climate_data)\n",
        "        df.to_sql('climate_data', conn, if_exists='replace', index=False)\n",
        "        conn.close()\n",
        "\n",
        "        print(f\"Generated {len(climate_data)} synthetic climate data points\")\n",
        "\n",
        "    def load_climate_data(self, region, start_date, end_date):\n",
        "        \"\"\"Load climate data for a specific region and time period\"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "\n",
        "        query = \"\"\"\n",
        "            SELECT * FROM climate_data\n",
        "            WHERE region = ? AND date BETWEEN ? AND ?\n",
        "            ORDER BY date\n",
        "        \"\"\"\n",
        "\n",
        "        df = pd.read_sql_query(query, conn, params=(region, start_date, end_date))\n",
        "        conn.close()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def calculate_historical_baseline(self, region):\n",
        "        \"\"\"Calculate historical baseline statistics for a region\"\"\"\n",
        "        # Get 30 years of historical data\n",
        "        end_date = \"2023-12-31\"\n",
        "        start_date = \"1994-01-01\"\n",
        "\n",
        "        df = self.load_climate_data(region, start_date, end_date)\n",
        "\n",
        "        baseline = {\n",
        "            'rainfall_mean': df['rainfall'].mean(),\n",
        "            'rainfall_std': df['rainfall'].std(),\n",
        "            'rainfall_25th': df['rainfall'].quantile(0.25),\n",
        "            'drought_threshold': df['rainfall'].quantile(self.config[\"drought_threshold\"]),\n",
        "            'temperature_mean': df['temperature'].mean(),\n",
        "            'ndvi_mean': df['ndvi'].mean()\n",
        "        }\n",
        "\n",
        "        return baseline\n",
        "\n",
        "    def prepare_quantum_circuit_qae(self, loss_function_params):\n",
        "        \"\"\"Prepare quantum circuit for Quantum Amplitude Estimation\"\"\"\n",
        "        # Create a quantum circuit for amplitude estimation\n",
        "        n_qubits = 6  # Number of qubits for estimation\n",
        "\n",
        "        # Create the A operator (state preparation)\n",
        "        A = QuantumCircuit(n_qubits)\n",
        "\n",
        "        # Prepare superposition state\n",
        "        for i in range(n_qubits - 1):\n",
        "            A.ry(loss_function_params[i % len(loss_function_params)], i)\n",
        "\n",
        "        # Create the Q operator (Grover-like operator)\n",
        "        Q = QuantumCircuit(n_qubits)\n",
        "\n",
        "        # Add controlled rotations based on loss function\n",
        "        for i in range(n_qubits - 1):\n",
        "            Q.cry(np.pi / 4, i, n_qubits - 1)\n",
        "\n",
        "        return A, Q\n",
        "\n",
        "    def quantum_amplitude_estimation(self, region_data):\n",
        "        \"\"\"Perform Quantum Amplitude Estimation for expected loss calculation\"\"\"\n",
        "        # Extract features for loss function\n",
        "        rainfall_anomaly = (region_data['rainfall'] - region_data['rainfall'].mean()) / region_data['rainfall'].std()\n",
        "\n",
        "        # Prepare loss function parameters\n",
        "        loss_params = [\n",
        "            np.clip(abs(anomaly), 0, np.pi/2) for anomaly in rainfall_anomaly[-30:]\n",
        "        ]\n",
        "\n",
        "        # Prepare quantum circuit\n",
        "        A, Q = self.prepare_quantum_circuit_qae(loss_params)\n",
        "\n",
        "        # Simulate quantum amplitude estimation\n",
        "        shots = self.config[\"qae_shots\"]\n",
        "\n",
        "        # Classical simulation of QAE result\n",
        "        # In practice, this would use actual QAE implementation\n",
        "        estimated_amplitude = np.mean([abs(p) for p in loss_params])\n",
        "\n",
        "        # Convert amplitude to expected loss\n",
        "        expected_loss = estimated_amplitude * self.config[\"max_payout\"]\n",
        "\n",
        "        # Calculate quantum advantage (speedup factor)\n",
        "        quantum_advantage = np.sqrt(shots) / shots  # Quadratic speedup\n",
        "\n",
        "        return expected_loss, quantum_advantage\n",
        "\n",
        "    def bayesian_posterior_update(self, prior_params, observed_data):\n",
        "        \"\"\"Update posterior distribution using Bayesian inference\"\"\"\n",
        "        # Simple Bayesian update for demonstration\n",
        "        # In practice, this would use more sophisticated methods\n",
        "\n",
        "        alpha_prior, beta_prior = prior_params\n",
        "\n",
        "        # Likelihood based on observed rainfall\n",
        "        likelihood = np.prod(stats.lognorm.pdf(observed_data, s=1, scale=50))\n",
        "\n",
        "        # Update parameters (simplified)\n",
        "        alpha_post = alpha_prior + len(observed_data)\n",
        "        beta_post = beta_prior + np.sum(observed_data)\n",
        "\n",
        "        return alpha_post, beta_post\n",
        "\n",
        "    def create_qubo_matrix(self, expected_loss, constraints):\n",
        "        \"\"\"Create QUBO matrix for pricing optimization\"\"\"\n",
        "        # Binary variables represent pricing decisions\n",
        "        # r1, r2: regional pricing bands\n",
        "        # l1, l2: loading factors\n",
        "        # b1, b2: reinsurance buffers\n",
        "\n",
        "        # Base QUBO matrix\n",
        "        Q = {\n",
        "            ('r1', 'r1'): 2.1,\n",
        "            ('r1', 'r2'): -0.3,\n",
        "            ('r2', 'r2'): 1.5,\n",
        "            ('l1', 'l1'): 1.9,\n",
        "            ('l2', 'l2'): 2.3,\n",
        "            ('r1', 'l1'): -0.4,\n",
        "            ('b1', 'b1'): 2.0,\n",
        "            ('b2', 'b2'): 2.2\n",
        "        }\n",
        "\n",
        "        # Adjust based on expected loss\n",
        "        loss_factor = expected_loss / 100  # Normalize\n",
        "\n",
        "        for key in Q:\n",
        "            Q[key] *= (1 + loss_factor)\n",
        "\n",
        "        # Add constraint penalties\n",
        "        if constraints.get('fairness_penalty'):\n",
        "            Q[('r1', 'r1')] += constraints['fairness_penalty']\n",
        "\n",
        "        if constraints.get('solvency_penalty'):\n",
        "            Q[('b1', 'b1')] += constraints['solvency_penalty']\n",
        "\n",
        "        return Q\n",
        "\n",
        "    def quantum_optimization_qubo(self, expected_loss, region):\n",
        "        \"\"\"Perform QUBO optimization for pricing decisions\"\"\"\n",
        "        # Define constraints\n",
        "        constraints = {\n",
        "            'fairness_penalty': 0.5,\n",
        "            'solvency_penalty': 0.3\n",
        "        }\n",
        "\n",
        "        # Create QUBO matrix\n",
        "        Q = self.create_qubo_matrix(expected_loss, constraints)\n",
        "\n",
        "        # Create binary quadratic model\n",
        "        bqm = dimod.BinaryQuadraticModel.from_qubo(Q)\n",
        "\n",
        "        # Use simulated annealing for demonstration\n",
        "        # In practice, this would use D-Wave's quantum annealer\n",
        "        sampler = SimulatedAnnealingSampler()\n",
        "\n",
        "        # Sample solutions\n",
        "        response = sampler.sample(bqm, num_reads=self.config[\"qubo_iterations\"])\n",
        "\n",
        "        # Get best solution\n",
        "        best_solution = response.first\n",
        "\n",
        "        # Convert binary solution to pricing parameters\n",
        "        pricing_config = self._decode_qubo_solution(best_solution.sample)\n",
        "\n",
        "        return pricing_config, best_solution.energy\n",
        "\n",
        "    def _decode_qubo_solution(self, binary_solution):\n",
        "        \"\"\"Decode QUBO binary solution to pricing parameters\"\"\"\n",
        "        base_premium = self.config[\"base_premium\"]\n",
        "\n",
        "        # Regional pricing multiplier\n",
        "        regional_multiplier = 1.0\n",
        "        if binary_solution.get('r1', 0) == 1:\n",
        "            regional_multiplier *= 1.2\n",
        "        if binary_solution.get('r2', 0) == 1:\n",
        "            regional_multiplier *= 1.1\n",
        "\n",
        "        # Loading factor\n",
        "        loading_factor = 1.0\n",
        "        if binary_solution.get('l1', 0) == 1:\n",
        "            loading_factor *= 1.15\n",
        "        if binary_solution.get('l2', 0) == 1:\n",
        "            loading_factor *= 1.1\n",
        "\n",
        "        # Reinsurance buffer\n",
        "        reinsurance_buffer = 0.0\n",
        "        if binary_solution.get('b1', 0) == 1:\n",
        "            reinsurance_buffer += 0.05\n",
        "        if binary_solution.get('b2', 0) == 1:\n",
        "            reinsurance_buffer += 0.03\n",
        "\n",
        "        premium = base_premium * regional_multiplier * loading_factor * (1 + reinsurance_buffer)\n",
        "\n",
        "        return {\n",
        "            'premium': premium,\n",
        "            'regional_multiplier': regional_multiplier,\n",
        "            'loading_factor': loading_factor,\n",
        "            'reinsurance_buffer': reinsurance_buffer\n",
        "        }\n",
        "\n",
        "    def classical_pricing_baseline(self, region_data):\n",
        "        \"\"\"Calculate classical baseline pricing for comparison\"\"\"\n",
        "        # Simple classical approach\n",
        "        rainfall_mean = region_data['rainfall'].mean()\n",
        "        rainfall_std = region_data['rainfall'].std()\n",
        "\n",
        "        # Risk score based on recent rainfall\n",
        "        recent_rainfall = region_data['rainfall'].tail(30).mean()\n",
        "        risk_score = max(0, (rainfall_mean - recent_rainfall) / rainfall_std)\n",
        "\n",
        "        # Classical premium calculation\n",
        "        classical_premium = self.config[\"base_premium\"] * (1 + risk_score * 0.2)\n",
        "\n",
        "        # Classical expected loss (simplified)\n",
        "        drought_prob = len(region_data[region_data['rainfall'] < region_data['rainfall'].quantile(0.25)]) / len(region_data)\n",
        "        classical_expected_loss = drought_prob * self.config[\"max_payout\"]\n",
        "\n",
        "        return classical_premium, classical_expected_loss\n",
        "\n",
        "    def process_region_pricing(self, region, current_date):\n",
        "        \"\"\"Process pricing for a specific region\"\"\"\n",
        "        # Load recent data (last 2 years)\n",
        "        start_date = (datetime.strptime(current_date, '%Y-%m-%d') - timedelta(days=730)).strftime('%Y-%m-%d')\n",
        "        region_data = self.load_climate_data(region, start_date, current_date)\n",
        "\n",
        "        if region_data.empty:\n",
        "            return None\n",
        "\n",
        "        # Calculate baseline statistics\n",
        "        baseline = self.calculate_historical_baseline(region)\n",
        "\n",
        "        # Quantum Amplitude Estimation for expected loss\n",
        "        quantum_expected_loss, quantum_advantage = self.quantum_amplitude_estimation(region_data)\n",
        "\n",
        "        # Bayesian posterior update\n",
        "        prior_params = (2.0, 1.0)  # Alpha, Beta for conjugate prior\n",
        "        posterior_params = self.bayesian_posterior_update(prior_params, region_data['rainfall'].values)\n",
        "\n",
        "        # QUBO optimization for pricing\n",
        "        pricing_config, optimization_energy = self.quantum_optimization_qubo(quantum_expected_loss, region)\n",
        "\n",
        "        # Classical baseline for comparison\n",
        "        classical_premium, classical_expected_loss = self.classical_pricing_baseline(region_data)\n",
        "\n",
        "        # Calculate payout probability\n",
        "        drought_events = len(region_data[region_data['rainfall'] < baseline['drought_threshold']])\n",
        "        payout_probability = drought_events / len(region_data)\n",
        "\n",
        "        result = {\n",
        "            'region': region,\n",
        "            'date': current_date,\n",
        "            'quantum_expected_loss': quantum_expected_loss,\n",
        "            'quantum_premium': pricing_config['premium'],\n",
        "            'classical_premium': classical_premium,\n",
        "            'classical_expected_loss': classical_expected_loss,\n",
        "            'payout_probability': payout_probability,\n",
        "            'quantum_advantage': quantum_advantage,\n",
        "            'optimization_energy': optimization_energy,\n",
        "            'pricing_config': pricing_config,\n",
        "            'baseline': baseline\n",
        "        }\n",
        "\n",
        "        return result\n",
        "\n",
        "    def run_full_pipeline(self, target_date=\"2024-01-01\"):\n",
        "        \"\"\"Run the complete quantum-enhanced pricing pipeline\"\"\"\n",
        "        print(\"Starting Quantum-Enhanced Dynamic Pricing Pipeline...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for region in self.config[\"regions\"]:\n",
        "            print(f\"\\nProcessing {region}...\")\n",
        "\n",
        "            try:\n",
        "                result = self.process_region_pricing(region, target_date)\n",
        "                if result:\n",
        "                    results.append(result)\n",
        "\n",
        "                    # Store results in database\n",
        "                    conn = sqlite3.connect(self.db_path)\n",
        "                    cursor = conn.cursor()\n",
        "\n",
        "                    cursor.execute('''\n",
        "                        INSERT INTO pricing_results\n",
        "                        (region, date, expected_loss, premium, payout_probability, quantum_advantage)\n",
        "                        VALUES (?, ?, ?, ?, ?, ?)\n",
        "                    ''', (\n",
        "                        result['region'],\n",
        "                        result['date'],\n",
        "                        result['quantum_expected_loss'],\n",
        "                        result['quantum_premium'],\n",
        "                        result['payout_probability'],\n",
        "                        result['quantum_advantage']\n",
        "                    ))\n",
        "\n",
        "                    conn.commit()\n",
        "                    conn.close()\n",
        "\n",
        "                    print(f\"  ✓ Quantum Premium: ${result['quantum_premium']:.2f}\")\n",
        "                    print(f\"  ✓ Classical Premium: ${result['classical_premium']:.2f}\")\n",
        "                    print(f\"  ✓ Expected Loss: ${result['quantum_expected_loss']:.2f}\")\n",
        "                    print(f\"  ✓ Payout Probability: {result['payout_probability']:.3f}\")\n",
        "                    print(f\"  ✓ Quantum Advantage: {result['quantum_advantage']:.6f}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"  ✗ Error processing {region}: {str(e)}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def evaluate_performance(self, results):\n",
        "        \"\"\"Evaluate performance metrics and comparison with classical methods\"\"\"\n",
        "        if not results:\n",
        "            return {}\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        quantum_premiums = [r['quantum_premium'] for r in results]\n",
        "        classical_premiums = [r['classical_premium'] for r in results]\n",
        "        expected_losses = [r['quantum_expected_loss'] for r in results]\n",
        "        payout_probs = [r['payout_probability'] for r in results]\n",
        "\n",
        "        # Pricing accuracy (how close premiums are to expected losses)\n",
        "        quantum_accuracy = 1 - np.mean([abs(p - e) / e for p, e in zip(quantum_premiums, expected_losses)])\n",
        "        classical_accuracy = 1 - np.mean([abs(p - e) / e for p, e in zip(classical_premiums, expected_losses)])\n",
        "\n",
        "        # Fairness metric (coefficient of variation)\n",
        "        quantum_fairness = 1 - (np.std(quantum_premiums) / np.mean(quantum_premiums))\n",
        "        classical_fairness = 1 - (np.std(classical_premiums) / np.mean(classical_premiums))\n",
        "\n",
        "        # Quantum advantage metrics\n",
        "        avg_quantum_advantage = np.mean([r['quantum_advantage'] for r in results])\n",
        "\n",
        "        metrics = {\n",
        "            'quantum_pricing_accuracy': quantum_accuracy,\n",
        "            'classical_pricing_accuracy': classical_accuracy,\n",
        "            'quantum_fairness': quantum_fairness,\n",
        "            'classical_fairness': classical_fairness,\n",
        "            'average_quantum_advantage': avg_quantum_advantage,\n",
        "            'pricing_improvement': (quantum_accuracy - classical_accuracy) / classical_accuracy * 100,\n",
        "            'fairness_improvement': (quantum_fairness - classical_fairness) / classical_fairness * 100\n",
        "        }\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def generate_report(self, results, metrics):\n",
        "        \"\"\"Generate comprehensive performance report\"\"\"\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"QUANTUM CLIMATE INSURANCE - PERFORMANCE REPORT\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        print(f\"\\nModel Execution Summary:\")\n",
        "        print(f\"- Regions Processed: {len(results)}\")\n",
        "        print(f\"- Successfully Executed: {len([r for r in results if r is not None])}\")\n",
        "        print(f\"- Average Quantum Advantage: {metrics.get('average_quantum_advantage', 0):.6f}\")\n",
        "\n",
        "        print(f\"\\nPerformance Comparison:\")\n",
        "        print(f\"- Quantum Pricing Accuracy: {metrics.get('quantum_pricing_accuracy', 0):.3f}\")\n",
        "        print(f\"- Classical Pricing Accuracy: {metrics.get('classical_pricing_accuracy', 0):.3f}\")\n",
        "        print(f\"- Pricing Improvement: {metrics.get('pricing_improvement', 0):.1f}%\")\n",
        "\n",
        "        print(f\"\\nFairness Analysis:\")\n",
        "        print(f\"- Quantum Fairness Score: {metrics.get('quantum_fairness', 0):.3f}\")\n",
        "        print(f\"- Classical Fairness Score: {metrics.get('classical_fairness', 0):.3f}\")\n",
        "        print(f\"- Fairness Improvement: {metrics.get('fairness_improvement', 0):.1f}%\")\n",
        "\n",
        "        print(f\"\\nRegional Results:\")\n",
        "        for result in results:\n",
        "            if result:\n",
        "                print(f\"- {result['region']}:\")\n",
        "                print(f\"    Quantum Premium: ${result['quantum_premium']:.2f}\")\n",
        "                print(f\"    Classical Premium: ${result['classical_premium']:.2f}\")\n",
        "                print(f\"    Expected Loss: ${result['quantum_expected_loss']:.2f}\")\n",
        "                print(f\"    Payout Probability: {result['payout_probability']:.3f}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def visualize_results(self, results):\n",
        "        \"\"\"Create visualizations of the results\"\"\"\n",
        "        if not results:\n",
        "            return\n",
        "\n",
        "        # Create comparison plots\n",
        "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "        # Premium comparison\n",
        "        regions = [r['region'] for r in results]\n",
        "        quantum_premiums = [r['quantum_premium'] for r in results]\n",
        "        classical_premiums = [r['classical_premium'] for r in results]\n",
        "\n",
        "        x = np.arange(len(regions))\n",
        "        width = 0.35\n",
        "\n",
        "        axes[0, 0].bar(x - width/2, quantum_premiums, width, label='Quantum', alpha=0.8)\n",
        "        axes[0, 0].bar(x + width/2, classical_premiums, width, label='Classical', alpha=0.8)\n",
        "        axes[0, 0].set_xlabel('Region')\n",
        "        axes[0, 0].set_ylabel('Premium ($)')\n",
        "        axes[0, 0].set_title('Premium Comparison: Quantum vs Classical')\n",
        "        axes[0, 0].set_xticks(x)\n",
        "        axes[0, 0].set_xticklabels(regions)\n",
        "        axes[0, 0].legend()\n",
        "\n",
        "        # Expected loss vs premium\n",
        "        expected_losses = [r['quantum_expected_loss'] for r in results]\n",
        "        axes[0, 1].scatter(expected_losses, quantum_premiums, label='Quantum', alpha=0.7)\n",
        "        axes[0, 1].scatter(expected_losses, classical_premiums, label='Classical', alpha=0.7)\n",
        "        axes[0, 1].plot([0, max(expected_losses)], [0, max(expected_losses)], 'k--', alpha=0.5)\n",
        "        axes[0, 1].set_xlabel('Expected Loss ($)')\n",
        "        axes[0, 1].set_ylabel('Premium ($)')\n",
        "        axes[0, 1].set_title('Premium vs Expected Loss')\n",
        "        axes[0, 1].legend()\n",
        "\n",
        "        # Payout probability distribution\n",
        "        payout_probs = [r['payout_probability'] for r in results]\n",
        "        axes[1, 0].bar(regions, payout_probs, alpha=0.7)\n",
        "        axes[1, 0].set_xlabel('Region')\n",
        "        axes[1, 0].set_ylabel('Payout Probability')\n",
        "        axes[1, 0].set_title('Payout Probability by Region')\n",
        "        axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Quantum advantage visualization\n",
        "        quantum_advantages = [r['quantum_advantage'] for r in results]\n",
        "        axes[1, 1].bar(regions, quantum_advantages, alpha=0.7, color='green')\n",
        "        axes[1, 1].set_xlabel('Region')\n",
        "        axes[1, 1].set_ylabel('Quantum Advantage')\n",
        "        axes[1, 1].set_title('Quantum Advantage by Region')\n",
        "        axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig('quantum_insurance_results.png', dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\nVisualization saved as 'quantum_insurance_results.png'\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main execution function\"\"\"\n",
        "    print(\"Quantum-Enhanced Climate Risk Insurance System\")\n",
        "    print(\"Team: Quantum HQ - Phase 3 Implementation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Initialize the system\n",
        "    system = QuantumClimateInsurance()\n",
        "\n",
        "    # Run the full pipeline\n",
        "    results = system.run_full_pipeline()\n",
        "\n",
        "    # Evaluate performance\n",
        "    metrics = system.evaluate_performance(results)\n",
        "\n",
        "    # Generate report\n",
        "    system.generate_report(results, metrics)\n",
        "\n",
        "    # Create visualizations\n",
        "    system.visualize_results(results)\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Pipeline execution completed successfully!\")\n",
        "    print(\"Check 'climate_insurance.db' for stored results.\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    return system, results, metrics\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    system, results, metrics = main()"
      ],
      "metadata": {
        "id": "vVjm84WFIaS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Quantum Utilities and Advanced Components\n",
        "Supporting quantum algorithms for climate risk insurance\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "from qiskit import QuantumCircuit, transpile, assemble\n",
        "from qiskit.circuit.library import RealAmplitudes, EfficientSU2\n",
        "from qiskit.quantum_info import Statevector, SparsePauliOp\n",
        "from qiskit_aer import AerSimulator\n",
        "from qiskit.algorithms import VQE, QAOA\n",
        "from qiskit.algorithms.optimizers import SPSA, COBYLA\n",
        "from qiskit.primitives import Estimator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class QuantumAmplitudeEstimator:\n",
        "    \"\"\"\n",
        "    Advanced Quantum Amplitude Estimation for loss function evaluation\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_qubits=6, shots=1024):\n",
        "        self.n_qubits = n_qubits\n",
        "        self.shots = shots\n",
        "        self.simulator = AerSimulator()\n",
        "\n",
        "    def create_loss_state_preparation(self, loss_parameters):\n",
        "        \"\"\"\n",
        "        Create state preparation circuit for loss function\n",
        "        \"\"\"\n",
        "        qc = QuantumCircuit(self.n_qubits)\n",
        "\n",
        "        # Create superposition of all possible states\n",
        "        for i in range(self.n_qubits - 1):\n",
        "            qc.ry(2 * np.arcsin(np.sqrt(loss_parameters[i % len(loss_parameters)])), i)\n",
        "\n",
        "        # Add entanglement\n",
        "        for i in range(self.n_qubits - 2):\n",
        "            qc.cx(i, i + 1)\n",
        "\n",
        "        return qc\n",
        "\n",
        "    def create_oracle(self, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Create oracle circuit for amplitude estimation\n",
        "        \"\"\"\n",
        "        qc = QuantumCircuit(self.n_qubits)\n",
        "\n",
        "        # Mark states above threshold\n",
        "        for i in range(self.n_qubits - 1):\n",
        "            qc.x(i)\n",
        "\n",
        "        # Multi-controlled Z gate\n",
        "        qc.mcx(list(range(self.n_qubits - 1)), self.n_qubits - 1)\n",
        "        qc.z(self.n_qubits - 1)\n",
        "        qc.mcx(list(range(self.n_qubits - 1)), self.n_qubits - 1)\n",
        "\n",
        "        for i in range(self.n_qubits - 1):\n",
        "            qc.x(i)\n",
        "\n",
        "        return qc\n",
        "\n",
        "    def grover_diffusion(self, state_prep_circuit):\n",
        "        \"\"\"\n",
        "        Create Grover diffusion operator\n",
        "        \"\"\"\n",
        "        qc = QuantumCircuit(self.n_qubits)\n",
        "\n",
        "        # Invert about average\n",
        "        qc.compose(state_prep_circuit.inverse(), inplace=True)\n",
        "\n",
        "        # Apply oracle to |0> state\n",
        "        qc.x(range(self.n_qubits))\n",
        "        qc.mcx(list(range(self.n_qubits - 1)), self.n_qubits - 1)\n",
        "        qc.z(self.n_qubits - 1)\n",
        "        qc.mcx(list(range(self.n_qubits - 1)), self.n_qubits - 1)\n",
        "        qc.x(range(self.n_qubits))\n",
        "\n",
        "        qc.compose(state_prep_circuit, inplace=True)\n",
        "\n",
        "        return qc\n",
        "\n",
        "    def estimate_amplitude(self, loss_parameters, iterations=3):\n",
        "        \"\"\"\n",
        "        Estimate amplitude using iterative quantum amplitude estimation\n",
        "        \"\"\"\n",
        "        # Create state preparation circuit\n",
        "        state_prep = self.create_loss_state_preparation(loss_parameters)\n",
        "\n",
        "        # Create oracle\n",
        "        oracle = self.create_oracle()\n",
        "\n",
        "        # Create full circuit\n",
        "        qc = QuantumCircuit(self.n_qubits, self.n_qubits)\n",
        "\n",
        "        # Apply state preparation\n",
        "        qc.compose(state_prep, inplace=True)\n",
        "\n",
        "        # Apply Grover iterations\n",
        "        diffusion = self.grover_diffusion(state_prep)\n",
        "\n",
        "        for _ in range(iterations):\n",
        "            qc.compose(oracle, inplace=True)\n",
        "            qc.compose(diffusion, inplace=True)\n",
        "\n",
        "        # Measure all qubits\n",
        "        qc.measure_all()\n",
        "\n",
        "        # Execute circuit\n",
        "        transpiled_qc = transpile(qc, self.simulator)\n",
        "        job = self.simulator.run(transpiled_qc, shots=self.shots)\n",
        "        result = job.result()\n",
        "        counts = result.get_counts()\n",
        "\n",
        "        # Calculate estimated amplitude\n",
        "        target_states = 0\n",
        "        total_counts = sum(counts.values())\n",
        "\n",
        "        for state, count in counts.items():\n",
        "            if state[-1] == '1':  # Target state marked by oracle\n",
        "                target_states += count\n",
        "\n",
        "        estimated_amplitude = np.sqrt(target_states / total_counts)\n",
        "\n",
        "        return estimated_amplitude\n",
        "\n",
        "class QuantumVariationalOptimizer:\n",
        "    \"\"\"\n",
        "    Variational Quantum Optimizer for insurance parameter optimization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_qubits=4, layers=3):\n",
        "        self.n_qubits = n_qubits\n",
        "        self.layers = layers\n",
        "        self.simulator = AerSimulator()\n",
        "        self.estimator = Estimator()\n",
        "\n",
        "    def create_ansatz(self):\n",
        "        \"\"\"\n",
        "        Create variational ansatz circuit\n",
        "        \"\"\"\n",
        "        return EfficientSU2(self.n_qubits, reps=self.layers)\n",
        "\n",
        "    def create_cost_hamiltonian(self, loss_data, premium_data):\n",
        "        \"\"\"\n",
        "        Create cost Hamiltonian for optimization\n",
        "        \"\"\"\n",
        "        # Create Pauli operators for cost function\n",
        "        pauli_list = []\n",
        "        coeffs = []\n",
        "\n",
        "        for i in range(self.n_qubits):\n",
        "            # Add individual qubit terms\n",
        "            pauli_str = ['I'] * self.n_qubits\n",
        "            pauli_str[i] = 'Z'\n",
        "            pauli_list.append(''.join(pauli_str))\n",
        "            coeffs.append(loss_data[i % len(loss_data)])\n",
        "\n",
        "        # Add interaction terms\n",
        "        for i in range(self.n_qubits - 1):\n",
        "            pauli_str = ['I'] * self.n_qubits\n",
        "            pauli_str[i] = 'Z'\n",
        "            pauli_str[i + 1] = 'Z'\n",
        "            pauli_list.append(''.join(pauli_str))\n",
        "            coeffs.append(premium_data[i % len(premium_data)] * 0.1)\n",
        "\n",
        "        return SparsePauliOp(pauli_list, coeffs)\n",
        "\n",
        "    def optimize_parameters(self, loss_data, premium_data, max_iter=100):\n",
        "        \"\"\"\n",
        "        Optimize variational parameters\n",
        "        \"\"\"\n",
        "        ansatz = self.create_ansatz()\n",
        "        hamiltonian = self.create_cost_hamiltonian(loss_data, premium_data)\n",
        "\n",
        "        # Use SPSA optimizer\n",
        "        optimizer = SPSA(maxiter=max_iter)\n",
        "\n",
        "        # Create VQE instance\n",
        "        vqe = VQE(self.estimator, ansatz, optimizer)\n",
        "\n",
        "        # Run optimization\n",
        "        result = vqe.compute_minimum_eigenvalue(hamiltonian)\n",
        "\n",
        "        return result\n",
        "\n",
        "class QuantumRiskAnalyzer:\n",
        "    \"\"\"\n",
        "    Quantum-enhanced risk analysis and portfolio optimization\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_assets=3, n_qubits=6):\n",
        "        self.n_assets = n_assets\n",
        "        self.n_qubits = n_qubits\n",
        "        self.simulator = AerSimulator()\n",
        "\n",
        "    def create_portfolio_circuit(self, weights):\n",
        "        \"\"\"\n",
        "        Create quantum circuit for portfolio representation\n",
        "        \"\"\"\n",
        "        qc = QuantumCircuit(self.n_qubits)\n",
        "\n",
        "        # Encode portfolio weights\n",
        "        for i, weight in enumerate(weights[:self.n_qubits]):\n",
        "            qc.ry(2 * np.arcsin(np.sqrt(weight)), i)\n",
        "\n",
        "        # Add correlation structure\n",
        "        for i in range(self.n_qubits - 1):\n",
        "            qc.cz(i, i + 1)\n",
        "\n",
        "        return qc\n",
        "\n",
        "    def calculate_quantum_risk(self, portfolio_weights, correlation_matrix):\n",
        "        \"\"\"\n",
        "        Calculate quantum-enhanced risk metrics\n",
        "        \"\"\"\n",
        "        qc = self.create_portfolio_circuit(portfolio_weights)\n",
        "\n",
        "        # Add measurement\n",
        "        qc.measure_all()\n",
        "\n",
        "        # Execute\n",
        "        transpiled_qc = transpile(qc, self.simulator)\n",
        "        job = self.simulator.run(transpiled_qc, shots=1024)\n",
        "        result = job.result()\n",
        "        counts = result.get_counts()\n",
        "\n",
        "        # Calculate risk metrics from measurement statistics\n",
        "        probabilities = np.array(list(counts.values())) / sum(counts.values())\n",
        "\n",
        "        # Quantum-enhanced variance calculation\n",
        "        variance = np.var(probabilities)\n",
        "\n",
        "        # Risk-adjusted return\n",
        "        expected_return = np.mean(probabilities)\n",
        "\n",
        "        return {\n",
        "            'quantum_variance': variance,\n",
        "            'expected_return': expected_return,\n",
        "            'sharpe_ratio': expected_return / np.sqrt(variance) if variance > 0 else 0\n",
        "        }\n",
        "\n",
        "class QuantumDataProcessor:\n",
        "    \"\"\"\n",
        "    Quantum data processing and feature extraction\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_features=8):\n",
        "        self.n_features = n_features\n",
        "        self.simulator = AerSimulator()\n",
        "\n",
        "    def quantum_feature_map(self, data):\n",
        "        \"\"\"\n",
        "        Create quantum feature map for classical data\n",
        "        \"\"\"\n",
        "        n_qubits = int(np.ceil(np.log2(len(data))))\n",
        "        qc = QuantumCircuit(n_qubits)\n",
        "\n",
        "        # Normalize data\n",
        "        normalized_data = (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "        # Encode data using rotation gates\n",
        "        for i, value in enumerate(normalized_data[:2**n_qubits]):\n",
        "            qubit_idx = i % n_qubits\n",
        "            qc.ry(2 * np.pi * value, qubit_idx)\n",
        "\n",
        "        # Add entanglement\n",
        "        for i in range(n_qubits - 1):\n",
        "            qc.cx(i, i + 1)\n",
        "\n",
        "        return qc\n",
        "\n",
        "    def quantum_kernel_estimation(self, data1, data2):\n",
        "        \"\"\"\n",
        "        Estimate quantum kernel between two datasets\n",
        "        \"\"\"\n",
        "        qc1 = self.quantum_feature_map(data1)\n",
        "        qc2 = self.quantum_feature_map(data2)\n",
        "\n",
        "        # Create kernel estimation circuit\n",
        "        n_qubits = qc1.num_qubits\n",
        "        kernel_qc = QuantumCircuit(n_qubits)\n",
        "\n",
        "        # Apply first feature map\n",
        "        kernel_qc.compose(qc1, inplace=True)\n",
        "\n",
        "        # Apply inverse of second feature map\n",
        "        kernel_qc.compose(qc2.inverse(), inplace=True)\n",
        "\n",
        "        # Measure overlap\n",
        "        kernel_qc.measure_all()\n",
        "\n",
        "        # Execute\n",
        "        transpiled_qc = transpile(kernel_qc, self.simulator)\n",
        "        job = self.simulator.run(transpiled_qc, shots=1024)\n",
        "        result = job.result()\n",
        "        counts = result.get_counts()\n",
        "\n",
        "        # Calculate kernel value (overlap probability)\n",
        "        zero_state_prob = counts.get('0' * n_qubits, 0) / 1024\n",
        "\n",
        "        return zero_state_prob\n",
        "\n",
        "def demonstrate_quantum_components():\n",
        "    \"\"\"\n",
        "    Demonstrate quantum components with sample data\n",
        "    \"\"\"\n",
        "    print(\"Demonstrating Quantum Components\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Sample climate data\n",
        "    np.random.seed(42)\n",
        "    rainfall_data = np.random.exponential(50, 30)\n",
        "    temperature_data = np.random.normal(25, 5, 30)\n",
        "\n",
        "    # 1. Quantum Amplitude Estimation\n",
        "    print(\"\\n1. Quantum Amplitude Estimation\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    qae = QuantumAmplitudeEstimator(n_qubits=6, shots=1024)\n",
        "\n",
        "    # Create loss parameters from climate data\n",
        "    loss_params = np.clip(rainfall_data / 100, 0, 1)[:6]\n",
        "\n",
        "    estimated_amplitude = qae.estimate_amplitude(loss_params, iterations=2)\n",
        "    print(f\"Estimated amplitude: {estimated_amplitude:.4f}\")\n",
        "\n",
        "    # 2. Quantum Variational Optimization\n",
        "    print(\"\\n2. Quantum Variational Optimization\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    qvo = QuantumVariationalOptimizer(n_qubits=4, layers=2)\n",
        "\n",
        "    # Sample loss and premium data\n",
        "    loss_data = np.random.exponential(0.1, 4)\n",
        "    premium_data = np.random.normal(0.05, 0.02, 4)\n",
        "\n",
        "    try:\n",
        "        opt_result = qvo.optimize_parameters(loss_data, premium_data, max_iter=10)\n",
        "        print(f\"Optimization eigenvalue: {opt_result.eigenvalue:.4f}\")\n",
        "        print(f\"Optimal parameters: {opt_result.optimal_parameters[:3]}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Optimization simulation: {e}\")\n",
        "\n",
        "    # 3. Quantum Risk Analysis\n",
        "    print(\"\\n3. Quantum Risk Analysis\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    qra = QuantumRiskAnalyzer(n_assets=3, n_qubits=6)\n",
        "\n",
        "    # Sample portfolio weights\n",
        "    weights = np.array([0.4, 0.3, 0.3, 0.2, 0.1, 0.1])\n",
        "    correlation_matrix = np.array([[1, 0.3, 0.2], [0.3, 1, 0.4], [0.2, 0.4, 1]])\n",
        "\n",
        "    risk_metrics = qra.calculate_quantum_risk(weights, correlation_matrix)\n",
        "    print(f\"Quantum variance: {risk_metrics['quantum_variance']:.4f}\")\n",
        "    print(f\"Expected return: {risk_metrics['expected_return']:.4f}\")\n",
        "    print(f\"Sharpe ratio: {risk_metrics['sharpe_ratio']:.4f}\")\n",
        "\n",
        "    # 4. Quantum Data Processing\n",
        "    print(\"\\n4. Quantum Data Processing\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    qdp = QuantumDataProcessor(n_features=8)\n",
        "\n",
        "    # Create feature map\n",
        "    feature_map = qdp.quantum_feature_map(rainfall_data[:8])\n",
        "    print(f\"Feature map circuit depth: {feature_map.depth()}\")\n",
        "    print(f\"Feature map circuit width: {feature_map.width()}\")\n",
        "\n",
        "    # Kernel estimation\n",
        "    kernel_value = qdp.quantum_kernel_estimation(rainfall_data[:8], temperature_data[:8])\n",
        "    print(f\"Quantum kernel value: {kernel_value:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"Quantum components demonstration completed!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    demonstrate_quantum_components()"
      ],
      "metadata": {
        "id": "V7aVqR-NI30a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data Pipeline and API Components\n",
        "Real-time data processing and web API for quantum climate insurance\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import requests\n",
        "import json\n",
        "import sqlite3\n",
        "from flask import Flask, jsonify, request\n",
        "from flask_cors import CORS\n",
        "import threading\n",
        "import time\n",
        "import logging\n",
        "from typing import Dict, List, Optional\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ClimateDataIngestion:\n",
        "    \"\"\"\n",
        "    Real-time climate data ingestion and processing\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, db_path=\"climate_insurance.db\"):\n",
        "        self.db_path = db_path\n",
        "        self.data_sources = {\n",
        "            'chirps': 'https://data.chc.ucsb.edu/products/CHIRPS-2.0',\n",
        "            'modis': 'https://modis.gsfc.nasa.gov/data',\n",
        "            'era5': 'https://cds.climate.copernicus.eu/cdsapp#!/dataset',\n",
        "            'ecmwf': 'https://www.ecmwf.int/en/forecasts/datasets'\n",
        "        }\n",
        "        self.update_interval = 3600  # 1 hour\n",
        "        self.running = False\n",
        "\n",
        "    def fetch_chirps_data(self, region_bounds, start_date, end_date):\n",
        "        \"\"\"\n",
        "        Fetch CHIRPS precipitation data (simulated)\n",
        "        \"\"\"\n",
        "        # Simulate CHIRPS data fetch\n",
        "        days = pd.date_range(start_date, end_date, freq='D')\n",
        "\n",
        "        # Generate realistic precipitation patterns\n",
        "        data = []\n",
        "        for day in days:\n",
        "            # Seasonal variation\n",
        "            day_of_year = day.timetuple().tm_yday\n",
        "            seasonal_factor = 1 + 0.8 * np.sin(2 * np.pi * day_of_year / 365)\n",
        "\n",
        "            # Random precipitation with log-normal distribution\n",
        "            precipitation = np.random.lognormal(\n",
        "                mean=np.log(5 * seasonal_factor),\n",
        "                sigma=0.8\n",
        "            )\n",
        "\n",
        "            data.append({\n",
        "                'date': day.strftime('%Y-%m-%d'),\n",
        "                'precipitation': precipitation,\n",
        "                'source': 'CHIRPS',\n",
        "                'region': region_bounds['name'],\n",
        "                'lat': region_bounds['lat'],\n",
        "                'lon': region_bounds['lon']\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def fetch_modis_ndvi(self, region_bounds, start_date, end_date):\n",
        "        \"\"\"\n",
        "        Fetch MODIS NDVI data (simulated)\n",
        "        \"\"\"\n",
        "        days = pd.date_range(start_date, end_date, freq='D')\n",
        "\n",
        "        data = []\n",
        "        for day in days:\n",
        "            # NDVI typically ranges from -1 to 1, vegetation usually 0.2-0.8\n",
        "            base_ndvi = 0.5 + 0.3 * np.sin(2 * np.pi * day.timetuple().tm_yday / 365)\n",
        "            ndvi = np.clip(base_ndvi + np.random.normal(0, 0.1), -1, 1)\n",
        "\n",
        "            data.append({\n",
        "                'date': day.strftime('%Y-%m-%d'),\n",
        "                'ndvi': ndvi,\n",
        "                'source': 'MODIS',\n",
        "                'region': region_bounds['name'],\n",
        "                'lat': region_bounds['lat'],\n",
        "                'lon': region_bounds['lon']\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def fetch_era5_temperature(self, region_bounds, start_date, end_date):\n",
        "        \"\"\"\n",
        "        Fetch ERA5 temperature data (simulated)\n",
        "        \"\"\"\n",
        "        days = pd.date_range(start_date, end_date, freq='D')\n",
        "\n",
        "        data = []\n",
        "        for day in days:\n",
        "            # Temperature with seasonal variation\n",
        "            day_of_year = day.timetuple().tm_yday\n",
        "            seasonal_temp = 20 + 15 * np.sin(2 * np.pi * day_of_year / 365)\n",
        "            temperature = seasonal_temp + np.random.normal(0, 3)\n",
        "\n",
        "            data.append({\n",
        "                'date': day.strftime('%Y-%m-%d'),\n",
        "                'temperature': temperature,\n",
        "                'source': 'ERA5',\n",
        "                'region': region_bounds['name'],\n",
        "                'lat': region_bounds['lat'],\n",
        "                'lon': region_bounds['lon']\n",
        "            })\n",
        "\n",
        "        return pd.DataFrame(data)\n",
        "\n",
        "    def calculate_drought_index(self, precipitation_data, historical_baseline):\n",
        "        \"\"\"\n",
        "        Calculate drought severity index\n",
        "        \"\"\"\n",
        "        # Standardized Precipitation Index (SPI) calculation\n",
        "        current_precip = precipitation_data['precipitation'].mean()\n",
        "        historical_mean = historical_baseline['precipitation_mean']\n",
        "        historical_std = historical_baseline['precipitation_std']\n",
        "\n",
        "        spi = (current_precip - historical_mean) / historical_std\n",
        "\n",
        "        # Convert to drought severity (0-1 scale, higher = more severe)\n",
        "        drought_severity = max(0, -spi / 3)  # Normalize to 0-1 range\n",
        "\n",
        "        return drought_severity\n",
        "\n",
        "    def process_and_store_data(self, region_config):\n",
        "        \"\"\"\n",
        "        Process and store climate data for a region\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Define date range (last 30 days)\n",
        "            end_date = datetime.now()\n",
        "            start_date = end_date - timedelta(days=30)\n",
        "\n",
        "            # Fetch data from different sources\n",
        "            chirps_data = self.fetch_chirps_data(\n",
        "                region_config,\n",
        "                start_date.strftime('%Y-%m-%d'),\n",
        "                end_date.strftime('%Y-%m-%d')\n",
        "            )\n",
        "\n",
        "            modis_data = self.fetch_modis_ndvi(\n",
        "                region_config,\n",
        "                start_date.strftime('%Y-%m-%d'),\n",
        "                end_date.strftime('%Y-%m-%d')\n",
        "            )\n",
        "\n",
        "            era5_data = self.fetch_era5_temperature(\n",
        "                region_config,\n",
        "                start_date.strftime('%Y-%m-%d'),\n",
        "                end_date.strftime('%Y-%m-%d')\n",
        "            )\n",
        "\n",
        "            # Merge data\n",
        "            merged_data = pd.merge(chirps_data, modis_data, on=['date', 'region'], how='outer')\n",
        "            merged_data = pd.merge(merged_data, era5_data, on=['date', 'region'], how='outer')\n",
        "\n",
        "            # Calculate additional indices\n",
        "            merged_data['precipitation_anomaly'] = (\n",
        "                merged_data['precipitation'] - merged_data['precipitation'].mean()\n",
        "            ) / merged_data['precipitation'].std()\n",
        "\n",
        "            # Store in database\n",
        "            conn = sqlite3.connect(self.db_path)\n",
        "\n",
        "            for _, row in merged_data.iterrows():\n",
        "                conn.execute('''\n",
        "                    INSERT OR REPLACE INTO climate_data\n",
        "                    (region, date, rainfall, temperature, ndvi, drought_index)\n",
        "                    VALUES (?, ?, ?, ?, ?, ?)\n",
        "                ''', (\n",
        "                    row['region'],\n",
        "                    row['date'],\n",
        "                    row['precipitation'],\n",
        "                    row['temperature'],\n",
        "                    row['ndvi'],\n",
        "                    abs(row['precipitation_anomaly'])  # Use as drought proxy\n",
        "                ))\n",
        "\n",
        "            conn.commit()\n",
        "            conn.close()\n",
        "\n",
        "            logger.info(f\"Processed {len(merged_data)} records for {region_config['name']}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error processing data for {region_config['name']}: {str(e)}\")\n",
        "\n",
        "    def start_data_ingestion(self, regions):\n",
        "        \"\"\"\n",
        "        Start continuous data ingestion\n",
        "        \"\"\"\n",
        "        def ingestion_loop():\n",
        "            while self.running:\n",
        "                for region in regions:\n",
        "                    self.process_and_store_data(region)\n",
        "                time.sleep(self.update_interval)\n",
        "\n",
        "        self.running = True\n",
        "        ingestion_thread = threading.Thread(target=ingestion_loop)\n",
        "        ingestion_thread.daemon = True\n",
        "        ingestion_thread.start()\n",
        "\n",
        "        logger.info(\"Data ingestion started\")\n",
        "\n",
        "    def stop_data_ingestion(self):\n",
        "        \"\"\"\n",
        "        Stop data ingestion\n",
        "        \"\"\"\n",
        "        self.running = False\n",
        "        logger.info(\"Data ingestion stopped\")\n",
        "\n",
        "class QuantumInsuranceAPI:\n",
        "    \"\"\"\n",
        "    Flask API for quantum insurance system\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, quantum_system, db_path=\"climate_insurance.db\"):\n",
        "        self.app = Flask(__name__)\n",
        "        CORS(self.app)\n",
        "        self.quantum_system = quantum_system\n",
        "        self.db_path = db_path\n",
        "        self.setup_routes()\n",
        "\n",
        "    def setup_routes(self):\n",
        "        \"\"\"\n",
        "        Setup API routes\n",
        "        \"\"\"\n",
        "\n",
        "        @self.app.route('/api/health', methods=['GET'])\n",
        "        def health_check():\n",
        "            \"\"\"Health check endpoint\"\"\"\n",
        "            return jsonify({\n",
        "                'status': 'healthy',\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'quantum_system': 'operational'\n",
        "            })\n",
        "\n",
        "        @self.app.route('/api/pricing', methods=['POST'])\n",
        "        def calculate_pricing():\n",
        "            \"\"\"Calculate quantum-enhanced pricing\"\"\"\n",
        "            try:\n",
        "                data = request.get_json()\n",
        "\n",
        "                region = data.get('region')\n",
        "                date = data.get('date', datetime.now().strftime('%Y-%m-%d'))\n",
        "\n",
        "                if not region:\n",
        "                    return jsonify({'error': 'Region is required'}), 400\n",
        "\n",
        "                # Calculate pricing using quantum system\n",
        "                result = self.quantum_system.process_region_pricing(region, date)\n",
        "\n",
        "                if result:\n",
        "                    return jsonify({\n",
        "                        'region': result['region'],\n",
        "                        'date': result['date'],\n",
        "                        'quantum_premium': result['quantum_premium'],\n",
        "                        'classical_premium': result['classical_premium'],\n",
        "                        'expected_loss': result['quantum_expected_loss'],\n",
        "                        'payout_probability': result['payout_probability'],\n",
        "                        'quantum_advantage': result['quantum_advantage']\n",
        "                    })\n",
        "                else:\n",
        "                    return jsonify({'error': 'Unable to calculate pricing'}), 500\n",
        "\n",
        "            except Exception as e:\n",
        "                return jsonify({'error': str(e)}), 500\n",
        "\n",
        "        @self.app.route('/api/risk-assessment', methods=['POST'])\n",
        "        def risk_assessment():\n",
        "            \"\"\"Perform quantum risk assessment\"\"\"\n",
        "            try:\n",
        "                data = request.get_json()\n",
        "\n",
        "                region = data.get('region')\n",
        "                portfolio_weights = data.get('weights', [0.4, 0.3, 0.3])\n",
        "\n",
        "                # Simplified risk assessment\n",
        "                conn = sqlite3.connect(self.db_path)\n",
        "                df = pd.read_sql_query(\n",
        "                    \"SELECT * FROM climate_data WHERE region = ? ORDER BY date DESC LIMIT 30\",\n",
        "                    conn, params=(region,)\n",
        "                )\n",
        "                conn.close()\n",
        "\n",
        "                if df.empty:\n",
        "                    return jsonify({'error': 'No data available for region'}), 404\n",
        "\n",
        "                # Calculate risk metrics\n",
        "                rainfall_volatility = df['rainfall'].std()\n",
        "                temperature_volatility = df['temperature'].std()\n",
        "\n",
        "                risk_score = (rainfall_volatility + temperature_volatility) / 2\n",
        "\n",
        "                return jsonify({\n",
        "                    'region': region,\n",
        "                    'risk_score': risk_score,\n",
        "                    'rainfall_volatility': rainfall_volatility,\n",
        "                    'temperature_volatility': temperature_volatility,\n",
        "                    'data_points': len(df)\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                return jsonify({'error': str(e)}), 500\n",
        "\n",
        "        @self.app.route('/api/data/latest', methods=['GET'])\n",
        "        def get_latest_data():\n",
        "            \"\"\"Get latest climate data\"\"\"\n",
        "            try:\n",
        "                region = request.args.get('region')\n",
        "                limit = int(request.args.get('limit', 10))\n",
        "\n",
        "                conn = sqlite3.connect(self.db_path)\n",
        "\n",
        "                if region:\n",
        "                    df = pd.read_sql_query(\n",
        "                        \"SELECT * FROM climate_data WHERE region = ? ORDER BY date DESC LIMIT ?\",\n",
        "                        conn, params=(region, limit)\n",
        "                    )\n",
        "                else:\n",
        "                    df = pd.read_sql_query(\n",
        "                        \"SELECT * FROM climate_data ORDER BY date DESC LIMIT ?\",\n",
        "                        conn, params=(limit,)\n",
        "                    )\n",
        "\n",
        "                conn.close()\n",
        "\n",
        "                return jsonify({\n",
        "                    'data': df.to_dict('records'),\n",
        "                    'count': len(df)\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                return jsonify({'error': str(e)}), 500\n",
        "\n",
        "        @self.app.route('/api/performance', methods=['GET'])\n",
        "        def get_performance_metrics():\n",
        "            \"\"\"Get system performance metrics\"\"\"\n",
        "            try:\n",
        "                # Get recent pricing results\n",
        "                conn = sqlite3.connect(self.db_path)\n",
        "                df = pd.read_sql_query(\n",
        "                    \"SELECT * FROM pricing_results ORDER BY date DESC LIMIT 100\",\n",
        "                    conn\n",
        "                )\n",
        "                conn.close()\n",
        "\n",
        "                if df.empty:\n",
        "                    return jsonify({'error': 'No performance data available'}), 404\n",
        "\n",
        "                # Calculate metrics\n",
        "                metrics = {\n",
        "                    'total_calculations': len(df),\n",
        "                    'average_quantum_advantage': df['quantum_advantage'].mean(),\n",
        "                    'average_premium': df['premium'].mean(),\n",
        "                    'average_expected_loss': df['expected_loss'].mean(),\n",
        "                    'regions_covered': df['region'].nunique()\n",
        "                }\n",
        "\n",
        "                return jsonify(metrics)\n",
        "\n",
        "            except Exception as e:\n",
        "                return jsonify({'error': str(e)}), 500\n",
        "\n",
        "        @self.app.route('/api/regions', methods=['GET'])\n",
        "        def get_regions():\n",
        "            \"\"\"Get available regions\"\"\"\n",
        "            return jsonify({\n",
        "                'regions': self.quantum_system.config['regions'],\n",
        "                'count': len(self.quantum_system.config['regions'])\n",
        "            })\n",
        "\n",
        "    def run(self, host='0.0.0.0', port=5000, debug=False):\n",
        "        \"\"\"\n",
        "        Run the Flask application\n",
        "        \"\"\"\n",
        "        logger.info(f\"Starting Quantum Insurance API on {host}:{port}\")\n",
        "        self.app.run(host=host, port=port, debug=debug)\n",
        "\n",
        "class DataQualityMonitor:\n",
        "    \"\"\"\n",
        "    Monitor data quality and system performance\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, db_path=\"climate_insurance.db\"):\n",
        "        self.db_path = db_path\n",
        "        self.quality_thresholds = {\n",
        "            'min_data_points': 20,\n",
        "            'max_missing_percentage': 0.1,\n",
        "            'rainfall_range': (0, 500),\n",
        "            'temperature_range': (-20, 60),\n",
        "            'ndvi_range': (-1, 1)\n",
        "        }\n",
        "\n",
        "    def check_data_quality(self, region=None):\n",
        "        \"\"\"\n",
        "        Check data quality for regions\n",
        "        \"\"\"\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "\n",
        "        if region:\n",
        "            df = pd.read_sql_query(\n",
        "                \"SELECT * FROM climate_data WHERE region = ?\",\n",
        "                conn, params=(region,)\n",
        "            )\n",
        "        else:\n",
        "            df = pd.read_sql_query(\"SELECT * FROM climate_data\", conn)\n",
        "\n",
        "        conn.close()\n",
        "\n",
        "        if df.empty:\n",
        "            return {'status': 'error', 'message': 'No data available'}\n",
        "\n",
        "        quality_report = {}\n",
        "\n",
        "        # Check data completeness\n",
        "        total_records = len(df)\n",
        "        missing_data = df.isnull().sum().sum()\n",
        "        missing_percentage = missing_data / (total_records * len(df.columns))\n",
        "\n",
        "        quality_report['completeness'] = {\n",
        "            'total_records': total_records,\n",
        "            'missing_values': int(missing_data),\n",
        "            'missing_percentage': missing_percentage,\n",
        "            'status': 'good' if missing_percentage < self.quality_thresholds['max_missing_percentage'] else 'warning'\n",
        "        }\n",
        "\n",
        "        # Check data ranges\n",
        "        for column, valid_range in [\n",
        "            ('rainfall', self.quality_thresholds['rainfall_range']),\n",
        "            ('temperature', self.quality_thresholds['temperature_range']),\n",
        "            ('ndvi', self.quality_thresholds['ndvi_range'])\n",
        "        ]:\n",
        "            if column in df.columns:\n",
        "                min_val, max_val = valid_range\n",
        "                outliers = len(df[(df[column] < min_val) | (df[column] > max_val)])\n",
        "\n",
        "                quality_report[f'{column}_range'] = {\n",
        "                    'outliers': outliers,\n",
        "                    'percentage': outliers / total_records,\n",
        "                    'status': 'good' if outliers < total_records * 0.05 else 'warning'\n",
        "                }\n",
        "\n",
        "        # Overall quality score\n",
        "        warning_count = sum(1 for metric in quality_report.values()\n",
        "                          if metric.get('status') == 'warning')\n",
        "\n",
        "        quality_report['overall'] = {\n",
        "            'score': max(0, 1 - warning_count / len(quality_report)),\n",
        "            'status': 'good' if warning_count == 0 else 'warning' if warning_count < 2 else 'poor'\n",
        "        }\n",
        "\n",
        "        return quality_report\n",
        "\n",
        "    def generate_quality_report(self):\n",
        "        \"\"\"\n",
        "        Generate comprehensive data quality report\n",
        "        \"\"\"\n",
        "        report = {\n",
        "            'timestamp': datetime.now().isoformat(),\n",
        "            'regions': {}\n",
        "        }\n",
        "\n",
        "        # Get all regions\n",
        "        conn = sqlite3.connect(self.db_path)\n",
        "        regions = pd.read_sql_query(\"SELECT DISTINCT region FROM climate_data\", conn)['region'].tolist()\n",
        "        conn.close()\n",
        "\n",
        "        for region in regions:\n",
        "            report['regions'][region] = self.check_data_quality(region)\n",
        "\n",
        "        # Overall system quality\n",
        "        overall_scores = [data['overall']['score'] for data in report['regions'].values()]\n",
        "        report['system_quality'] = {\n",
        "            'average_score': np.mean(overall_scores),\n",
        "            'regions_count': len(regions),\n",
        "            'healthy_regions': sum(1 for score in overall_scores if score > 0.8)\n",
        "        }\n",
        "\n",
        "        return report\n",
        "\n",
        "def demonstrate_data_pipeline():\n",
        "    \"\"\"\n",
        "    Demonstrate the data pipeline components\n",
        "    \"\"\"\n",
        "    print(\"Demonstrating Data Pipeline Components\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Sample region configurations\n",
        "    regions = [\n",
        "        {'name': 'Region_A', 'lat': 40.7128, 'lon': -74.0060},\n",
        "        {'name': 'Region_B', 'lat': 34.0522, 'lon': -118.2437},\n",
        "        {'name': 'Region_C', 'lat': 41.8781, 'lon': -87.6298}\n",
        "    ]\n",
        "\n",
        "    # 1. Data Ingestion\n",
        "    print(\"\\n1. Climate Data Ingestion\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    ingestion = ClimateDataIngestion()\n",
        "\n",
        "    # Process sample data\n",
        "    for region in regions:\n",
        "        ingestion.process_and_store_data(region)\n",
        "\n",
        "    print(\"Data ingestion completed\")\n",
        "\n",
        "    # 2. Data Quality Monitoring\n",
        "    print(\"\\n2. Data Quality Monitoring\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    monitor = DataQualityMonitor()\n",
        "    quality_report = monitor.generate_quality_report()\n",
        "\n",
        "    print(f\"System quality score: {quality_report['system_quality']['average_score']:.3f}\")\n",
        "    print(f\"Regions monitored: {quality_report['system_quality']['regions_count']}\")\n",
        "    print(f\"Healthy regions: {quality_report['system_quality']['healthy_regions']}\")\n",
        "\n",
        "    # 3. API Demonstration\n",
        "    print(\"\\n3. API Components\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    # This would normally start the Flask app\n",
        "    print(\"API endpoints configured:\")\n",
        "    print(\"- /api/health - Health check\")\n",
        "    print(\"- /api/pricing - Quantum pricing calculation\")\n",
        "    print(\"- /api/risk-assessment - Regional climate risk score\")\n",
        "    print(\"- /api/data/latest - Recent climate data\")\n",
        "    print(\"- /api/performance - System performance metrics\")\n",
        "    print(\"- /api/regions - Configured insurance regions\")\n",
        "\n",
        "    print(\"\\nUse `QuantumInsuranceAPI(...).run()` to launch the API.\")\n"
      ],
      "metadata": {
        "id": "Gf-X-qf7Jux7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mock QuantumSystem with basic logic for now\n",
        "class MockQuantumSystem:\n",
        "    def __init__(self):\n",
        "        self.config = {'regions': ['Region_A', 'Region_B', 'Region_C']}\n",
        "\n",
        "    def process_region_pricing(self, region, date):\n",
        "        return {\n",
        "            'region': region,\n",
        "            'date': date,\n",
        "            'quantum_premium': round(np.random.uniform(20, 100), 2),\n",
        "            'classical_premium': round(np.random.uniform(20, 100), 2),\n",
        "            'quantum_expected_loss': round(np.random.uniform(10, 80), 2),\n",
        "            'payout_probability': round(np.random.uniform(0.1, 0.9), 2),\n",
        "            'quantum_advantage': round(np.random.uniform(-5, 15), 2)\n",
        "        }\n",
        "\n",
        "# Initialize and run the API\n",
        "quantum_system = MockQuantumSystem()\n",
        "api = QuantumInsuranceAPI(quantum_system)\n",
        "api.run(debug=True)  # Launches the server\n"
      ],
      "metadata": {
        "id": "wbv0ZgPdLrBP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}